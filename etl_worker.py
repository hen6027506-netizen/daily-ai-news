import os
import requests
import json
import time
from bs4 import BeautifulSoup
from supabase import create_client, Client
import google.generativeai as genai # ğŸ‘ˆ æ›æˆ Google å¥—ä»¶

# === 1. è¨­å®šèˆ‡é€£ç·š ===
SUPABASE_URL = os.environ.get("SUPABASE_URL")
SUPABASE_KEY = os.environ.get("SUPABASE_KEY")
GOOGLE_API_KEY = os.environ.get("GOOGLE_API_KEY") # ğŸ‘ˆ è®€å– Google Key

if not SUPABASE_URL or not SUPABASE_KEY or not GOOGLE_API_KEY:
    print("âŒ éŒ¯èª¤ï¼šæ‰¾ä¸åˆ°ç’°å¢ƒè®Šæ•¸ (è«‹æª¢æŸ¥ GitHub Secrets)")
    exit(1)

# åˆå§‹åŒ–
supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)
genai.configure(api_key=GOOGLE_API_KEY)
model = genai.GenerativeModel('gemini-1.5-flash') # ğŸ‘ˆ ä½¿ç”¨å…è²»ä¸”å¿«é€Ÿçš„æ¨¡å‹

# === 2. çˆ¬èŸ²å‡½æ•¸ ===
def fetch_latest_news():
    print("ğŸ” æ­£åœ¨æœå°‹ TechCrunch æ–°è...")
    url = "https://techcrunch.com/"
    try:
        response = requests.get(url)
        soup = BeautifulSoup(response.text, "html.parser")
        articles = []
        
        # æŠ“å–å‰ 3 ç¯‡æ–°è (é¿å…é¡åº¦ç”¨å®Œ)
        for item in soup.select(".loop-card__title a")[:3]:
            title = item.get_text().strip()
            link = item.get("href")
            
            # ç°¡å–®é˜²é‡è¦†æª¢æŸ¥
            existing = supabase.table("news_items").select("id").eq("original_url", link).execute()
            if not existing.data:
                articles.append({"title": title, "url": link})
        
        print(f"âœ… æ‰¾åˆ° {len(articles)} ç¯‡æ–°æ–‡ç« ")
        return articles
    except Exception as e:
        print(f"âŒ çˆ¬èŸ²å¤±æ•—: {e}")
        return []

# === 3. AI åˆ†æå‡½æ•¸ (Google ç‰ˆ) ===
def analyze_with_gemini(text):
    print("ğŸ¤– AI æ­£åœ¨é–±è®€...")
    prompt = f"""
    ä½ æ˜¯å°ˆæ¥­çš„ç§‘æŠ€æ–°èç·¨è¼¯ã€‚è«‹é–±è®€ä»¥ä¸‹æ–°èå…§å®¹ï¼Œä¸¦è¼¸å‡ºç´” JSON æ ¼å¼çš„åˆ†æçµæœã€‚
    
    æ ¼å¼è¦æ±‚ï¼š
    {{
        "summary_short": "50å­—ä»¥å…§çš„ç¹é«”ä¸­æ–‡æ‘˜è¦ï¼Œèªæ°£è¦å¸å¼•äºº",
        "summary_detailed": "æ¢åˆ—å¼é‡é»ï¼ˆç¹é«”ä¸­æ–‡ï¼‰",
        "sentiment_score": 0.5,
        "tags": ["æ¨™ç±¤1", "æ¨™ç±¤2"]
    }}

    æ–°èå…§å®¹ï¼š
    {text[:2000]}
    """
    
    try:
        response = model.generate_content(prompt)
        content = response.text
        
        # æ¸…ç† Gemini å¯èƒ½æœƒè¼¸å‡ºçš„ Markdown ç¬¦è™Ÿ
        content = content.replace("```json", "").replace("```", "").strip()
        
        return json.loads(content)
    except Exception as e:
        print(f"âŒ AI åˆ†æå¤±æ•—: {e}")
        return None

# === 4. ä¸»ç¨‹å¼ ===
def main():
    news_list = fetch_latest_news()
    
    for news in news_list:
        print(f"è™•ç†ä¸­: {news['title']}")
        
        # 1. å­˜å…¥ news_items
        news_data = {
            "title": news['title'],
            "source_name": "TechCrunch",
            "published_at": time.strftime('%Y-%m-%d'),
            "original_url": news['url']
        }
        result = supabase.table("news_items").insert(news_data).execute()
        news_id = result.data[0]['id']
        
        # 2. é€²è¡Œ AI åˆ†æ
        ai_result = analyze_with_gemini(news['title']) # ç°¡å–®æ¸¬è©¦ç”¨æ¨™é¡Œåˆ†æ
        
        if ai_result:
            # 3. å­˜å…¥ ai_analysis
            analysis_data = {
                "news_id": news_id,
                "summary_short": ai_result.get("summary_short"),
                "summary_detailed": str(ai_result.get("summary_detailed")),
                "sentiment_score": ai_result.get("sentiment_score", 0),
                "tags": ai_result.get("tags", [])
            }
            supabase.table("ai_analysis").insert(analysis_data).execute()
            print("âœ… å¯«å…¥è³‡æ–™åº«æˆåŠŸï¼")

if __name__ == "__main__":
    main()