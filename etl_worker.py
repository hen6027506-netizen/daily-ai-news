import os
import requests
import json
import time
from bs4 import BeautifulSoup
from supabase import create_client, Client
import google.generativeai as genai

# === 1. è¨­å®šèˆ‡é€£ç·š ===
SUPABASE_URL = os.environ.get("SUPABASE_URL")
SUPABASE_KEY = os.environ.get("SUPABASE_KEY")
GOOGLE_API_KEY = os.environ.get("GOOGLE_API_KEY")

if not SUPABASE_URL or not SUPABASE_KEY or not GOOGLE_API_KEY:
    print("âŒ éŒ¯èª¤ï¼šæ‰¾ä¸åˆ°ç’°å¢ƒè®Šæ•¸")
    exit(1)

supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)
genai.configure(api_key=GOOGLE_API_KEY)

target_model = "models/gemini-flash-latest" 
print(f"ğŸ”’ å¼·åˆ¶é–å®šæ¨¡å‹: {target_model}")
model = genai.GenerativeModel(target_model)

# === ğŸš€ æ–°èä¾†æºæ¸…å–® ===
NEWS_SOURCES = [
    {"name": "TechCrunch", "url": "https://techcrunch.com/feed/"},
    {"name": "Yahoo Finance", "url": "https://finance.yahoo.com/news/rssindex"},
    {"name": "BBC World", "url": "http://feeds.bbci.co.uk/news/world/rss.xml"},
    {"name": "ScienceDaily", "url": "https://www.sciencedaily.com/rss/all.xml"}
]

HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8"
}

# === 2. æŠ“å–å‡½æ•¸ ===
def fetch_all_news():
    all_articles = []
    print(f"ğŸŒ é–‹å§‹å·¡é‚ {len(NEWS_SOURCES)} å€‹æ–°èé »é“...")
    
    for source in NEWS_SOURCES:
        try:
            print(f"ğŸ“¡ æ­£åœ¨é€£ç·š: {source['name']}...")
            response = requests.get(source['url'], headers=HEADERS, timeout=10)
            
            if response.status_code != 200:
                print(f"âš ï¸ {source['name']} æ‹’çµ•é€£ç·š (ä»£ç¢¼: {response.status_code})")
                continue

            soup = BeautifulSoup(response.content, "xml") 
            items = soup.find_all("item")[:1] # æ¯å€‹ä¾†æºåªæŠ“ 1 ç¯‡
            
            for item in items:
                title = item.title.text.strip() if item.title else ""
                link = item.link.text.strip() if item.link else ""
                
                if not title or not link: continue

                existing = supabase.table("news_items").select("id").eq("original_url", link).execute()
                if not existing.data:
                    all_articles.append({
                        "title": title,
                        "url": link,
                        "source_name": source['name']
                    })
        except Exception as e:  # ğŸ‘ˆ é€™å°±æ˜¯ä¹‹å‰æ¼æ‰çš„é—œéµæ•‘å‘½ç¬¦ï¼
            print(f"âŒ æŠ“å– {source['name']} ç™¼ç”ŸéŒ¯èª¤: {e}")
    
    print(f"ğŸ‰ å·¡é‚å®Œç•¢ï¼Œå…±ç™¼ç¾ {len(all_articles)} ç¯‡æ–°æ–‡ç« ")
    return all_articles

# === 3. AI åˆ†æå‡½æ•¸ ===
def analyze_with_gemini(title):
    print(f"ğŸ¤– AI åˆ†æä¸­ (ä½¿ç”¨ {target_model}): {title[:20]}...")
    prompt = f"""
    è«‹é–±è®€æ–°èæ¨™é¡Œï¼š"{title}"
    è«‹ç›´æ¥è¼¸å‡º JSONï¼š
    {{
        "category": "è«‹å¾ [ç§‘æŠ€, è²¡ç¶“, ç§‘å­¸, ç”Ÿæ´»] ä¸­æŒ‘é¸ä¸€å€‹æœ€åˆé©çš„",
        "summary_short": "50å­—ä»¥å…§çš„ç¹é«”ä¸­æ–‡æ‘˜è¦",
        "summary_detailed": "æ¢åˆ—å¼é‡é»ï¼ˆç¹é«”ä¸­æ–‡ï¼‰",
        "sentiment_score": 0.5,
        "tags": ["æ¨™ç±¤1", "æ¨™ç±¤2"]
    }}
    """
    try:
        response = model.generate_content(prompt)
        content = response.text.replace("```json", "").replace("```", "").strip()
        return json.loads(content)
    except Exception as e:
        print(f"âŒ AI æ€è€ƒå¤±æ•—: {e}")
        return None

# === 4. ä¸»ç¨‹å¼ ===
def main():
    articles = fetch_all_news()
    
    if not articles:
        print("ğŸ˜´ æ²’æœ‰ç™¼ç¾ä»»ä½•æ–°æ–°èï¼Œæ”¶å·¥ï¼")
        return

    for i, art in enumerate(articles):
        if i > 0:
            print("â˜• ä¼‘æ¯ 5 ç§’...")
            time.sleep(5)

        news_data = {
            "title": art['title'],
            "source_name": art['source_name'],
            "published_at": time.strftime('%Y-%m-%d'),
            "original_url": art['url'],
            "processing_status": "pending"
        }
        try:
            res = supabase.table("news_items").insert(news_data).execute()
            if not res.data: continue
            news_id = res.data[0]['id']
            
            ai_res = analyze_with_gemini(art['title'])
            
            if ai_res:
                supabase.table("news_items").update({
                    "category": ai_res.get("category", "ç§‘æŠ€"),
                    "processing_status": "complete"
                }).eq("id", news_id).execute()
                
                analysis_data = {
                    "news_id": news_id,
                    "summary_short": ai_res.get("summary_short"),
                    "summary_detailed": str(ai_res.get("summary_detailed")),
                    "sentiment_score": ai_res.get("sentiment_score", 0),
                    "tags": ai_res.get("tags", [])
                }
                supabase.table("ai_analysis").insert(analysis_data).execute()
                print("   âœ… è³‡æ–™åº«å¯«å…¥å®Œæˆ")
                
        except Exception as e:
            print(f"âš ï¸ è™•ç†å¤±æ•—: {e}")

if __name__ == "__main__":
    main()
