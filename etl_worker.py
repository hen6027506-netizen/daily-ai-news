import os
import requests
import json
import time
from datetime import datetime, timedelta # ğŸ‘ˆ è¨˜å¾—å¼•å…¥é€™å…©å€‹æ™‚é–“å·¥å…·
from bs4 import BeautifulSoup
from supabase import create_client, Client
import google.generativeai as genai

# === 1. è¨­å®šèˆ‡é€£ç·š ===
SUPABASE_URL = os.environ.get("SUPABASE_URL")
SUPABASE_KEY = os.environ.get("SUPABASE_KEY")
GOOGLE_API_KEY = os.environ.get("GOOGLE_API_KEY")
LINE_NOTIFY_TOKEN = os.environ.get("LINE_NOTIFY_TOKEN")

if not SUPABASE_URL or not SUPABASE_KEY or not GOOGLE_API_KEY:
    print("âŒ éŒ¯èª¤ï¼šæ‰¾ä¸åˆ°ç’°å¢ƒè®Šæ•¸")
    exit(1)

supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)
genai.configure(api_key=GOOGLE_API_KEY)

target_model = "models/gemini-flash-latest"
model = genai.GenerativeModel(target_model)
WEB_APP_URL = "https://ä½ çš„ç¶²å€.vercel.app" 

NEWS_SOURCES = [
    {"name": "TechCrunch", "url": "https://techcrunch.com/feed/"},
    {"name": "Yahoo Finance", "url": "https://finance.yahoo.com/news/rssindex"},
    {"name": "BBC World", "url": "http://feeds.bbci.co.uk/news/world/rss.xml"},
    {"name": "ScienceDaily", "url": "https://www.sciencedaily.com/rss/all.xml"}
]

HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8"
}

def send_line_notify(msg):
    if not LINE_NOTIFY_TOKEN: return
    headers = {"Authorization": "Bearer " + LINE_NOTIFY_TOKEN}
    try:
        requests.post("https://notify-api.line.me/api/notify", headers=headers, data={"message": msg})
    except: pass

def fetch_all_news():
    all_articles = []
    print(f"ğŸŒ é–‹å§‹å·¡é‚ {len(NEWS_SOURCES)} å€‹æ–°èé »é“...")
    for source in NEWS_SOURCES:
        try:
            print(f"ğŸ“¡ æ­£åœ¨é€£ç·š: {source['name']}...")
            response = requests.get(source['url'], headers=HEADERS, timeout=10)
            if response.status_code != 200: continue
            soup = BeautifulSoup(response.content, "xml") 
            items = soup.find_all("item")[:1] 
            for item in items:
                title = item.title.text.strip() if item.title else ""
                link = item.link.text.strip() if item.link else ""
                if not title or not link: continue
                existing = supabase.from("news_items").select("id").eq("original_url", link).execute()
                if not existing.data:
                    all_articles.append({"title": title, "url": link, "source_name": source['name']})
        except Exception as e:
            print(f"âŒ éŒ¯èª¤: {e}")
    return all_articles

def analyze_with_gemini(title):
    print(f"ğŸ¤– AI åˆ†æä¸­: {title[:20]}...")
    prompt = f"""
    è«‹é–±è®€æ–°èæ¨™é¡Œï¼š"{title}"
    é€™æ˜¯ä¸€å€‹è‹±èªå­¸ç¿’æ–°èç¶²ç«™ã€‚è«‹åˆ†æä¸¦è¼¸å‡º JSONï¼š
    {{
        "category": "è«‹å¾ [ç§‘æŠ€, è²¡ç¶“, ç§‘å­¸, ç”Ÿæ´»] ä¸­æŒ‘é¸ä¸€å€‹æœ€åˆé©çš„",
        "summary_short": "50å­—ä»¥å…§çš„ç¹é«”ä¸­æ–‡æ‘˜è¦",
        "sentiment_score": 0.5,
        "tags": ["æ¨™ç±¤1"],
        "vocabulary": [
            {{ "word": "å–®å­—", "def": "å®šç¾©", "ex": "ä¾‹å¥" }}
        ]
    }}
    è«‹æŒ‘é¸ 3 å€‹ç›¸é—œè‹±æ–‡å–®å­—ã€‚
    """
    try:
        response = model.generate_content(prompt)
        content = response.text.replace("```json", "").replace("```", "").strip()
        return json.loads(content)
    except: return None

# === ğŸ§¹ è‡ªå‹•æ¸…ç†åŠŸèƒ½ (æ–°å¢) ===
def cleanup_old_news():
    print("ğŸ§¹ é–‹å§‹åŸ·è¡ŒéæœŸæ–°èæ¸…ç†...")
    
    # è¨­å®šæœŸé™ï¼š30 å¤©å‰
    days_to_keep = 30
    cutoff_date = (datetime.now() - timedelta(days=days_to_keep)).strftime('%Y-%m-%d')
    
    try:
        # é‚è¼¯ï¼šåˆªé™¤ (æ—¥æœŸ < 30å¤©å‰) AND (is_saved ç‚º FALSE æˆ– NULL)
        # æ³¨æ„ï¼šSupabase çš„ delete æœƒé€£å‹•åˆªé™¤ ai_analysis (å¦‚æœæœ‰è¨­å®š foreign key cascade)
        # å¦‚æœæ²’æœ‰ cascadeï¼Œå¯èƒ½æœƒæœ‰æ®˜ç•™è³‡æ–™ï¼Œä½†é€šå¸¸ä¸æœƒå½±éŸ¿é‹ä½œ
        res = supabase.from("news_items").delete() \
            .lt("created_at", cutoff_date) \
            .eq("is_saved", False) \
            .execute()
        
        # æª¢æŸ¥åˆªäº†å¹¾ç­† (Supabase å›å‚³çµæ§‹ data è£¡å°±æ˜¯åˆªé™¤çš„é …ç›®)
        deleted_count = len(res.data) if res.data else 0
        
        if deleted_count > 0:
            print(f"â™»ï¸ æˆåŠŸæ¸…ç†äº† {deleted_count} ç¯‡æœªæ”¶è—çš„éæœŸæ–°è")
        else:
            print("âœ¨ ç³»çµ±å¾ˆä¹¾æ·¨ï¼Œæ²’æœ‰éœ€è¦æ¸…ç†çš„éæœŸæ–°è")
            
    except Exception as e:
        print(f"âš ï¸ æ¸…ç†éç¨‹ç™¼ç”ŸéŒ¯èª¤: {e}")

def main():
    articles = fetch_all_news()
    if articles:
        success_count = 0
        for i, art in enumerate(articles):
            if i > 0: time.sleep(5)
            news_data = {
                "title": art['title'],
                "source_name": art['source_name'],
                "published_at": time.strftime('%Y-%m-%d'),
                "original_url": art['url'],
                "processing_status": "pending",
                "is_saved": False # é è¨­ä¸æ”¶è—
            }
            try:
                res = supabase.from("news_items").insert(news_data).execute()
                if not res.data: continue
                news_id = res.data[0]['id']
                ai_res = analyze_with_gemini(art['title'])
                if ai_res:
                    supabase.from("news_items").update({
                        "category": ai_res.get("category", "ç§‘æŠ€"),
                        "processing_status": "complete"
                    }).eq("id", news_id).execute()
                    
                    analysis_data = {
                        "news_id": news_id,
                        "summary_short": ai_res.get("summary_short"),
                        "sentiment_score": ai_res.get("sentiment_score", 0),
                        "tags": ai_res.get("tags", []),
                        "vocabulary": ai_res.get("vocabulary", [])
                    }
                    supabase.from("ai_analysis").insert(analysis_data).execute()
                    print("   âœ… è³‡æ–™åº«å¯«å…¥å®Œæˆ")
                    success_count += 1
            except Exception as e: print(f"âš ï¸ å¤±æ•—: {e}")

        if success_count > 0:
            send_line_notify(f"\n\nğŸ‡¬ğŸ‡§ è‹±èªå­¸ç¿’æ—¥å ±å·²å‡ºåˆŠï¼\n\nğŸ“Š æ–°å¢ï¼š{success_count} ç¯‡\nğŸ”— {WEB_APP_URL}")
    
    # ğŸƒâ€â™‚ï¸ æ¯æ¬¡è·‘å®Œæ–°èæŠ“å–å¾Œï¼Œé †ä¾¿æ‰“æƒç’°å¢ƒ
    cleanup_old_news()

if __name__ == "__main__":
    main()